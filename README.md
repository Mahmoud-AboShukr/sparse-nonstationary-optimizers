Sparse and Non-Stationary Gradients in Large-Scale Language Model Optimization

This research project investigates how different optimizers behave under sparse and non-stationary gradient regimes during the training of transformer-based language models (LMs). The aim is to build a diagnostic framework capable of revealing optimizer robustness, failure modes, and adaptation behaviour in realistic language modeling scenarios.

ğŸ“Œ Project Overview

Large-scale language models frequently encounter two challenging gradient properties:

Sparse gradients â€” Only a small subset of parameters are updated at each training step, especially in embedding layers and rare tokens.

Non-stationary gradients â€” Gradient statistics drift over time as the optimization landscape evolves, making historical momentum and variance estimates potentially stale.

Classical optimizers like SGD fail to cope with these dynamics. Adaptive optimizers such as AdamW, Adafactor, and Lion improve stability, but their behaviour under combined sparsity + drift has not been systematically analysed.
This project fills that gap.

ğŸ¯ Research Objectives

Quantify gradient sparsity across different Transformer layers

Measure gradient drift and historical gradient aging

Compare optimizer behaviour under identical training conditions

Identify which optimizers:

remain stable under sparse update regimes

adapt efficiently to shifting gradient distributions

accumulate stale momentum or variance estimates

ğŸ—ï¸ Project Structure
sparse_nonstationary_optimizers/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/          # dataset loaders (WikiText-103)
â”‚   â”œâ”€â”€ models/        # GPT-style transformer implementation
â”‚   â”œâ”€â”€ optimizers/    # AdamW, Adafactor, SGD, Lion wrappers
â”‚   â”œâ”€â”€ metrics/       # sparsity + non-stationarity diagnostic tools
â”‚   â””â”€â”€ utils/         # helper functions
â”‚
â”œâ”€â”€ configs/           # experiment configuration files
â”œâ”€â”€ logs/              # raw metric output
â”œâ”€â”€ results/           # processed experimental outputs
â”œâ”€â”€ plots/             # visualizations and comparison graphs
â””â”€â”€ train.py           # main training entry point

ğŸš€ Getting Started
1ï¸âƒ£ Create and activate a virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

2ï¸âƒ£ Install dependencies
pip install torch transformers datasets sentencepiece accelerate wandb

3ï¸âƒ£ Run a test experiment
python train.py


You should see a confirmation message indicating the experiment framework initialized successfully.

ğŸ§ª Experiments

This project evaluates optimizers along two core axes:

Property	Diagnostics
Sparsity	Update frequency, embedding sparsity, parameter activation rate
Non-stationarity	Gradient drift, vâ‚œ statistics aging, layer-wise norm evolution
Target Optimizers

SGD + Momentum â€” baseline for failure under sparse gradients

AdamW â€” industry standard

Adafactor â€” memory-efficient and sparse-friendly

Lion (optional) â€” sign-based update behaviour

ğŸ‘¤ Author
Ramzi Amira
Mohammed Abo Shukr
M2 MMVAI â€” UniversitÃ© Paris-Saclay
2025

ğŸ“„ License

This repository is part of an academic research project. Redistribution is permitted with attribution.
